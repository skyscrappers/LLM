{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-29T12:23:55.095814Z","iopub.status.busy":"2024-09-29T12:23:55.094857Z","iopub.status.idle":"2024-09-29T12:24:07.713730Z","shell.execute_reply":"2024-09-29T12:24:07.712488Z","shell.execute_reply.started":"2024-09-29T12:23:55.095761Z"},"trusted":true},"outputs":[],"source":["!pip install accelerate --quiet"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:24:07.716293Z","iopub.status.busy":"2024-09-29T12:24:07.715721Z","iopub.status.idle":"2024-09-29T12:24:12.388764Z","shell.execute_reply":"2024-09-29T12:24:12.387954Z","shell.execute_reply.started":"2024-09-29T12:24:07.716257Z"},"trusted":true},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import json\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:24:12.390305Z","iopub.status.busy":"2024-09-29T12:24:12.389887Z","iopub.status.idle":"2024-09-29T12:25:05.182392Z","shell.execute_reply":"2024-09-29T12:25:05.180761Z","shell.execute_reply.started":"2024-09-29T12:24:12.390272Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6e1aaa9a4f34bc3b9165ea127d35cdd","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89b79d6c3beb4b9fb97e527d10a1b79b","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4ee74b4f13d4e2181d15f948f98fbec","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9b8392f4a2448aea0c2efa90d7376a4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"206834a75deb4f44ba906e24ad72f74f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a645ed9b09f94bec9245808e99b40840","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd8cbead259e49519325cfcdcbc077b9","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"398f68ec772544538d3397e21cb37d06","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"008ef61eb2504e929d50e138629cb062","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"382f47e015694dc199f00566e1fd1605","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da60e3a33a4546fa9d6fe200f342eaff","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n","model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:25:08.236042Z","iopub.status.busy":"2024-09-29T12:25:08.235654Z","iopub.status.idle":"2024-09-29T12:25:08.367030Z","shell.execute_reply":"2024-09-29T12:25:08.365831Z","shell.execute_reply.started":"2024-09-29T12:25:08.236007Z"},"trusted":true},"outputs":[],"source":["data = json.load(open(\"/kaggle/input/visionpulse-inference/BLIP_IIITD.json\"))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:25:22.629013Z","iopub.status.busy":"2024-09-29T12:25:22.628616Z","iopub.status.idle":"2024-09-29T12:25:51.916541Z","shell.execute_reply":"2024-09-29T12:25:51.915610Z","shell.execute_reply.started":"2024-09-29T12:25:22.628977Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/24 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","100%|██████████| 24/24 [00:29<00:00,  1.22s/it]\n"]}],"source":["for i in tqdm(range(len(data))):\n","    Question = data[i]['question']\n","    Reference_answers = ', '.join([x for x in data[i]['answers']])\n","    Candidate_answer = data[i]['prediction']\n","    PROMPT= f\"Task description: \\\n","           You are given a question, a set of gold-standard reference answers written by experts, and a candidate answer. Please rate the accuracy of the candidate answer for the question considering the reference answers. Use a scale of 1-3, with 1 indicating an incorrect or irrelevant answer, 2 indicating an ambiguous or incomplete answer, and 3 indicating a correct answer. Only give the rating. \\\n","           Demonstrations: \\\n","    Question: What is the color of the car? \\\n","    Reference answers: red, red, red, red, scarlet \\\n","    Candidate answer: pink \\\n","    Output: The candidate answer is incorrect because the car is red and not pink. So rating=1 \\\n","    Question: What is the animal on the left? \\\n","    Reference answers: elephant, giraffe, giraffe, giraffe, giraffe \\\n","    Candidate answer: giraffe \\\n","    Output: The candidate answer is correct because most of the reference answers (4 out of 5) indicate the \\\n","        animal on the left is a giraffe. So rating=3 \\\n","    Question: Whats the weather like? \\\n","    Reference answers: bright, bright and sunny, clear, sunny, sunny, sunny \\\n","    Candidate answer: cloudy \\\n","    Output: The candidate answer is incorrect because the weather is bright and sunny, not cloudy. So \\\n","    rating=1 \\\n","    Question: What are the people in the picture doing? \\\n","    Reference answers: sitting, sitting, sitting, sitting \\\n","    Candidate answer: they are resting \\\n","    Output: The candidate answer is ambiguous because, while it is common that people who are sitting are \\\n","        resting, it is not always the case. So rating=2 \\\n","    Question: What color are the base tiles? \\\n","    Reference answers: beige, beige, beige, brown, brown, tan, tan, tan, tan, ten \\\n","    Candidate answer: brown \\\n","    Output: The candidate answer is correct because the reference answers include brown and other similar \\\n","        colors such as tan or beige. So rating=3 \\\n","    Question: How many people are in the picture? \\\n","    Reference answers: four, three, three, three, two, two \\\n","    Candidate answer: a few \\\n","    Output: The candidate answer is incomplete because a few is less specific than the numerical reference \\\n","        answers. So rating=2 \\\n","    Question: What type of fruit is in the picture? \\\n","    Reference answers: apple \\\n","    Candidate answer: fruit \\\n","    Output: The candidate answer is incorrect because it does not specify the type of fruit. So rating=1 \\\n","    Question: What type of sculpture is this? \\\n","    Reference answers: Horse statue. \\\n","    Candidate answer: horse \\\n","    Output: The candidate answer is correct because horse is equivalent to horse statue in this context. \\\n","    So rating=3 \\\n","    Test example:\\\n","    Question:{Question}\\\n","    Reference answers: {Reference_answers}\\\n","    Candidate answer:{Candidate_answer}\\\n","    Output:\"\n","    input_ids = tokenizer(PROMPT, return_tensors=\"pt\").input_ids.to(\"cuda\")\n","    outputs = model.generate(input_ids)\n","    data[i]['LAVE'] = tokenizer.decode(outputs[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:26:15.529393Z","iopub.status.busy":"2024-09-29T12:26:15.529043Z","iopub.status.idle":"2024-09-29T12:26:15.534970Z","shell.execute_reply":"2024-09-29T12:26:15.534009Z","shell.execute_reply.started":"2024-09-29T12:26:15.529361Z"},"trusted":true},"outputs":[],"source":["json.dump(data, open(f'BLIP_IIITD_LAVE.json', 'w'), indent=4)"]},{"cell_type":"markdown","metadata":{},"source":["----------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:26:03.098231Z","iopub.status.busy":"2024-09-29T12:26:03.097192Z","iopub.status.idle":"2024-09-29T12:26:03.105658Z","shell.execute_reply":"2024-09-29T12:26:03.104604Z","shell.execute_reply.started":"2024-09-29T12:26:03.098185Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'incorrect': 0.6666666666666666, 'ambiguous': 0.08333333333333333, 'correct': 0.25}\n"]}],"source":["import json\n","data = json.load(open(\"BLIP_IIITD_LAVE.json\"))\n","LAVE = [x['LAVE'] for x in data]\n","\n","count, lave_score = {'1': 0, '2': 0, '3': 0}, {}\n","for i in LAVE:\n","    count[i] += 1\n","lave_score['incorrect'] = count['1']/len(LAVE)\n","lave_score['ambiguous'] = count['2']/len(LAVE)\n","lave_score['correct'] = count['3']/len(LAVE)\n","\n","print(lave_score)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5784183,"sourceId":9508634,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
