{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-29T12:01:33.704109Z","iopub.status.busy":"2024-09-29T12:01:33.703796Z","iopub.status.idle":"2024-09-29T12:01:46.798521Z","shell.execute_reply":"2024-09-29T12:01:46.797210Z","shell.execute_reply.started":"2024-09-29T12:01:33.704075Z"},"trusted":true},"outputs":[],"source":["!pip install accelerate --quiet"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:01:46.801377Z","iopub.status.busy":"2024-09-29T12:01:46.800950Z","iopub.status.idle":"2024-09-29T12:01:52.251405Z","shell.execute_reply":"2024-09-29T12:01:52.250608Z","shell.execute_reply.started":"2024-09-29T12:01:46.801331Z"},"trusted":true},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import json\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:01:52.252930Z","iopub.status.busy":"2024-09-29T12:01:52.252510Z","iopub.status.idle":"2024-09-29T12:02:45.809216Z","shell.execute_reply":"2024-09-29T12:02:45.808381Z","shell.execute_reply.started":"2024-09-29T12:01:52.252897Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edc6ae2456dd4d8b85e5cbb321266aeb","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0a001ea02104f02859561e913979e6f","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"225276039f8146a38ecf1aa4d76e1801","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7ec217178fa4cd89f6f5924f3fef855","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"389d36647a6f42f1a545d6b597310e8c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7156bf7224b948df8a497791f4e7ba6a","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c11ced9748b4c60a9d552899e1775e2","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"730849862e8d419d83a99d1a7ccfbef3","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e1ce9d051524907a96f8d4dfe8b81b4","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d111130a87cc47e68e4604074c91c764","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a04d326b059493eb2efeffc49fe2ee8","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n","model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:02:51.996472Z","iopub.status.busy":"2024-09-29T12:02:51.996089Z","iopub.status.idle":"2024-09-29T12:02:52.002394Z","shell.execute_reply":"2024-09-29T12:02:52.001481Z","shell.execute_reply.started":"2024-09-29T12:02:51.996435Z"},"trusted":true},"outputs":[],"source":["data = json.load(open(\"/kaggle/input/visionpulse-inference/Llama3_2_IIITD.json\"))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:03:53.540048Z","iopub.status.busy":"2024-09-29T12:03:53.539663Z","iopub.status.idle":"2024-09-29T12:04:23.408317Z","shell.execute_reply":"2024-09-29T12:04:23.407391Z","shell.execute_reply.started":"2024-09-29T12:03:53.540015Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/24 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n","100%|██████████| 24/24 [00:29<00:00,  1.24s/it]\n"]}],"source":["for i in tqdm(range(len(data))):\n","    Question = data[i]['question']\n","    Reference_answers = ', '.join([x for x in data[i]['answers']])\n","    Candidate_answer = data[i]['prediction']\n","    PROMPT= f\"Task description: \\\n","           You are given a question, a set of gold-standard reference answers written by experts, and a candidate answer. Please rate the accuracy of the candidate answer for the question considering the reference answers. Use a scale of 1-3, with 1 indicating an incorrect or irrelevant answer, 2 indicating an ambiguous or incomplete answer, and 3 indicating a correct answer. Only give the rating. \\\n","           Demonstrations: \\\n","    Question: What is the color of the car? \\\n","    Reference answers: red, red, red, red, scarlet \\\n","    Candidate answer: pink \\\n","    Output: The candidate answer is incorrect because the car is red and not pink. So rating=1 \\\n","    Question: What is the animal on the left? \\\n","    Reference answers: elephant, giraffe, giraffe, giraffe, giraffe \\\n","    Candidate answer: giraffe \\\n","    Output: The candidate answer is correct because most of the reference answers (4 out of 5) indicate the \\\n","        animal on the left is a giraffe. So rating=3 \\\n","    Question: Whats the weather like? \\\n","    Reference answers: bright, bright and sunny, clear, sunny, sunny, sunny \\\n","    Candidate answer: cloudy \\\n","    Output: The candidate answer is incorrect because the weather is bright and sunny, not cloudy. So \\\n","    rating=1 \\\n","    Question: What are the people in the picture doing? \\\n","    Reference answers: sitting, sitting, sitting, sitting \\\n","    Candidate answer: they are resting \\\n","    Output: The candidate answer is ambiguous because, while it is common that people who are sitting are \\\n","        resting, it is not always the case. So rating=2 \\\n","    Question: What color are the base tiles? \\\n","    Reference answers: beige, beige, beige, brown, brown, tan, tan, tan, tan, ten \\\n","    Candidate answer: brown \\\n","    Output: The candidate answer is correct because the reference answers include brown and other similar \\\n","        colors such as tan or beige. So rating=3 \\\n","    Question: How many people are in the picture? \\\n","    Reference answers: four, three, three, three, two, two \\\n","    Candidate answer: a few \\\n","    Output: The candidate answer is incomplete because a few is less specific than the numerical reference \\\n","        answers. So rating=2 \\\n","    Question: What type of fruit is in the picture? \\\n","    Reference answers: apple \\\n","    Candidate answer: fruit \\\n","    Output: The candidate answer is incorrect because it does not specify the type of fruit. So rating=1 \\\n","    Question: What type of sculpture is this? \\\n","    Reference answers: Horse statue. \\\n","    Candidate answer: horse \\\n","    Output: The candidate answer is correct because horse is equivalent to horse statue in this context. \\\n","    So rating=3 \\\n","    Test example:\\\n","    Question:{Question}\\\n","    Reference answers: {Reference_answers}\\\n","    Candidate answer:{Candidate_answer}\\\n","    Output:\"\n","    input_ids = tokenizer(PROMPT, return_tensors=\"pt\").input_ids.to(\"cuda\")\n","    outputs = model.generate(input_ids)\n","    data[i]['LAVE'] = tokenizer.decode(outputs[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:05:14.643792Z","iopub.status.busy":"2024-09-29T12:05:14.643359Z","iopub.status.idle":"2024-09-29T12:05:14.650067Z","shell.execute_reply":"2024-09-29T12:05:14.649066Z","shell.execute_reply.started":"2024-09-29T12:05:14.643754Z"},"trusted":true},"outputs":[],"source":["json.dump(data, open(f'Llama3_2_IIITD_LAVE.json', 'w'), indent=4)"]},{"cell_type":"markdown","metadata":{},"source":["----------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T12:04:48.124355Z","iopub.status.busy":"2024-09-29T12:04:48.123236Z","iopub.status.idle":"2024-09-29T12:04:48.130820Z","shell.execute_reply":"2024-09-29T12:04:48.129860Z","shell.execute_reply.started":"2024-09-29T12:04:48.124313Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'incorrect': 0.2916666666666667, 'ambiguous': 0.3333333333333333, 'correct': 0.375}\n"]}],"source":["import json\n","data = json.load(open(\"Llama3_2_IIITD_LAVE.json\"))\n","LAVE = [x['LAVE'] for x in data]\n","\n","count, lave_score = {'1': 0, '2': 0, '3': 0}, {}\n","for i in LAVE:\n","    count[i] += 1\n","lave_score['incorrect'] = count['1']/len(LAVE)\n","lave_score['ambiguous'] = count['2']/len(LAVE)\n","lave_score['correct'] = count['3']/len(LAVE)\n","\n","print(lave_score)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5784183,"sourceId":9508494,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
