{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4e16efbd44744c4fbfdc047af35f6f89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f901426e7d444bfb3a9f88dc3c2b0f0","IPY_MODEL_cd73176cf66b4f7593f22e65cbb54849","IPY_MODEL_1e284d16da18460b998d8b315b5df088"],"layout":"IPY_MODEL_d208d9405a5d46c186e362c58816df45"}},"5f901426e7d444bfb3a9f88dc3c2b0f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a570885d9b7e4d77ba1e3d93e435d51d","placeholder":"​","style":"IPY_MODEL_398878ca09d0416685b1a53df3aecdba","value":"Loading checkpoint shards:  67%"}},"cd73176cf66b4f7593f22e65cbb54849":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7ded2597e974f3b884f1110506d0358","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b2ff7ebada74163a0b0477fc047954a","value":2}},"1e284d16da18460b998d8b315b5df088":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6209f9d51454514bce02663c9df4561","placeholder":"​","style":"IPY_MODEL_fd5838fbd81149c1a82a95b5c3a850b2","value":" 2/3 [00:40&lt;00:20, 20.41s/it]"}},"d208d9405a5d46c186e362c58816df45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a570885d9b7e4d77ba1e3d93e435d51d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"398878ca09d0416685b1a53df3aecdba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7ded2597e974f3b884f1110506d0358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b2ff7ebada74163a0b0477fc047954a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6209f9d51454514bce02663c9df4561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd5838fbd81149c1a82a95b5c3a850b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"652608c923444078a0abfd0924ab9198":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_439b3e23320e40c99adf0daaa01c7495","IPY_MODEL_20fb588367134f2795425b7863919b8e","IPY_MODEL_59b6e9889ea347beaf1a3d1a2b14fcf8"],"layout":"IPY_MODEL_c0d3c6fbfb8c4f5eba3e60a09fbcc95a"}},"439b3e23320e40c99adf0daaa01c7495":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc8a348594104f8aa66a3c06a3356bb6","placeholder":"​","style":"IPY_MODEL_40d1aa4f611b43229fed3ce1eccfbb70","value":"Loading checkpoint shards: 100%"}},"20fb588367134f2795425b7863919b8e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d837f23f7f724a56ba7550c3688ef3fc","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_880408fce56c45a7b627a3403dbe1a4e","value":4}},"59b6e9889ea347beaf1a3d1a2b14fcf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3052f708c8f84655a65a8417c1c34035","placeholder":"​","style":"IPY_MODEL_6a6e9ecbb9f249ef9a1a93d3eeeee2a0","value":" 4/4 [00:56&lt;00:00, 10.66s/it]"}},"c0d3c6fbfb8c4f5eba3e60a09fbcc95a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc8a348594104f8aa66a3c06a3356bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40d1aa4f611b43229fed3ce1eccfbb70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d837f23f7f724a56ba7550c3688ef3fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"880408fce56c45a7b627a3403dbe1a4e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3052f708c8f84655a65a8417c1c34035":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a6e9ecbb9f249ef9a1a93d3eeeee2a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9287956,"sourceType":"datasetVersion","datasetId":5622530},{"sourceId":9288434,"sourceType":"datasetVersion","datasetId":5622846},{"sourceId":9290474,"sourceType":"datasetVersion","datasetId":5624371}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers huggingface_hub\n!pip install bitsandbytes\n\nfrom huggingface_hub import login\n\n# Login with your API token\nlogin(\"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\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMKXGA7Pi1cz","outputId":"9efd581d-9f5b-4cf9-b99b-201ef80720d9","execution":{"iopub.status.busy":"2024-09-01T15:57:55.123093Z","iopub.execute_input":"2024-09-01T15:57:55.123833Z","iopub.status.idle":"2024-09-01T15:58:27.258499Z","shell.execute_reply.started":"2024-09-01T15:57:55.123785Z","shell.execute_reply":"2024-09-01T15:58:27.257472Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# importing the packages\nimport torch\nimport warnings\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n\nwarnings.filterwarnings('ignore')","metadata":{"id":"udAmSwOmZmTw","execution":{"iopub.status.busy":"2024-09-01T15:58:39.168748Z","iopub.execute_input":"2024-09-01T15:58:39.169638Z","iopub.status.idle":"2024-09-01T15:58:43.067601Z","shell.execute_reply.started":"2024-09-01T15:58:39.169585Z","shell.execute_reply":"2024-09-01T15:58:43.066817Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215,"referenced_widgets":["652608c923444078a0abfd0924ab9198","439b3e23320e40c99adf0daaa01c7495","20fb588367134f2795425b7863919b8e","59b6e9889ea347beaf1a3d1a2b14fcf8","c0d3c6fbfb8c4f5eba3e60a09fbcc95a","fc8a348594104f8aa66a3c06a3356bb6","40d1aa4f611b43229fed3ce1eccfbb70","d837f23f7f724a56ba7550c3688ef3fc","880408fce56c45a7b627a3403dbe1a4e","3052f708c8f84655a65a8417c1c34035","6a6e9ecbb9f249ef9a1a93d3eeeee2a0"]},"id":"y-bltgspjJ8J","outputId":"07715569-5e8a-4519-e24a-a7d746845f09","execution":{"iopub.status.busy":"2024-09-01T15:58:54.829270Z","iopub.execute_input":"2024-09-01T15:58:54.830545Z","iopub.status.idle":"2024-09-01T16:00:56.773324Z","shell.execute_reply.started":"2024-09-01T15:58:54.830491Z","shell.execute_reply":"2024-09-01T16:00:56.772321Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a217c823b92b4dbdbfcfe3486c689c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77265f6eab1842ba84f0787a53f22959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8be55c3e057498898ba06955699e401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a559f975884e4cf5bab31f503a0c5d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b0acbe51b3945df88c9463088b912c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc69d9db95f46a7a8d858ad5b67d4fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5bbf1102bc64fa69cb53a38e1983d53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea976afaf37847db8916d99ccc4e500d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2b115df221477eb78d370ab8ddc87e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"209c7f25eb024557999cb93279060e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302574a0224f4dbf9b760ecdef8868e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c58bc48798984e7bb32360abc8c57ec4"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport pickle\nimport pandas as pd\n# Extract the model from the pipeline\nmodel = pipeline.model\ntokenizer = pipeline.tokenizer\n\n# Ensure the model outputs hidden states\nmodel.config.output_hidden_states = True\nmodel.eval()\n\n# Move the model to GPU if available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# Function to extract final token embedding using the existing pipeline setup\ndef extract_token_embeddings(prompt):\n    # Tokenize the input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Perform a forward pass with hidden states\n    with torch.no_grad():\n        outputs = model(**inputs, output_hidden_states=True)\n    \n    # Extract all hidden states\n    hidden_states = outputs.hidden_states\n    \n    # Convert hidden states from bfloat16 to float32\n    hidden_states = [state.to(torch.float32) for state in hidden_states]\n    \n    # Get the first, middle, and last hidden states\n    first_hidden_state = hidden_states[0]  # First layer\n    mid_hidden_state = hidden_states[len(hidden_states) // 2]  # Middle layer\n    last_hidden_state = hidden_states[-1]  # Final layer\n    \n    # Extract the final token's embedding from each of these layers\n    first_token_embedding = first_hidden_state[:, -1, :].squeeze().cpu().numpy()\n    mid_token_embedding = mid_hidden_state[:, -1, :].squeeze().cpu().numpy()\n    final_token_embedding = last_hidden_state[:, -1, :].squeeze().cpu().numpy()\n    \n    return first_token_embedding, mid_token_embedding, final_token_embedding\n\n\ncsv_file = '/kaggle/input/classreg/classification.csv'  # Path to your CSV file\nregression_df = pd.read_csv(csv_file)\n\n# Take only the 'prompt' column and get the first 50 samples\nregression_prompts = regression_df['classification_prompt'].tolist()[:400]\n\nfirst_layer_embeddings = []\nmid_layer_embeddings = []\nfinal_layer_embeddings = []\n\n# Generate embeddings for the first 50 prompts\nfor prompt in regression_prompts:\n    first_emb, mid_emb, final_emb = extract_token_embeddings(prompt)\n    first_layer_embeddings.append(first_emb)\n    mid_layer_embeddings.append(mid_emb)\n    final_layer_embeddings.append(final_emb)\n\n# Save the embeddings to separate pickle files\nwith open('1_classification.pkl', 'wb') as f:\n    pickle.dump(first_layer_embeddings, f)\n\nwith open('2_classification.pkl', 'wb') as f:\n    pickle.dump(mid_layer_embeddings, f)\n\nwith open('3_classification.pkl', 'wb') as f:\n    pickle.dump(final_layer_embeddings, f)\n\n# Print confirmation message\nprint(\"Embeddings have been generated and saved to 'first_layer_embeddings.pkl', 'mid_layer_embeddings.pkl', and 'final_layer_embeddings.pkl'.\")\n# prompt = \"Tell me about the movie The Shining released in 1980. It is directed by Stanley Kubrick and stars Jack Nicholson. What type of movie is this?\"\n\n# # Generate the embedding for the single prompt\n# embedding = extract_final_token_embedding(prompt)\n\n# # Print the embedding\n# # print(len(embedding))\n# csv_file = '/kaggle/input/regression-prompt/regression_prompts_runtime.csv'  # Replace with your CSV file path\n# classification_df = pd.read_csv(csv_file)\n\n# # Example: Extract embeddings for prompts from the classification dataframe\n# classification_prompts = classification_df['regression_prompt'].tolist()[:50]\n\n# # Generate embeddings for the first 50 prompts\n# embeddings = [extract_final_token_embedding(prompt) for prompt in classification_prompts]\n\n# # Save the embeddings to a pickle file\n# with open('embeddings_regression.pkl', 'wb') as f:\n#     pickle.dump(embeddings, f)\n\n# # Print confirmation message\n# print(f\"Generated embeddings for {len(embeddings)} prompts and saved them to 'embeddings_regression.pkl'.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"DMH71ua_VlMu","outputId":"e533fc7d-0bc9-49d7-82ce-eae3852f3f74","execution":{"iopub.status.busy":"2024-09-01T16:16:12.771963Z","iopub.execute_input":"2024-09-01T16:16:12.772745Z","iopub.status.idle":"2024-09-01T16:21:58.475900Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Embeddings have been generated and saved to 'first_layer_embeddings.pkl', 'mid_layer_embeddings.pkl', and 'final_layer_embeddings.pkl'.\n","output_type":"stream"}]},{"cell_type":"code","source":"messages = [\n    # {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"which is bigger, 9.11 and 9.9\"},\n    {\"role\": \"user\", \"content\": \"Select two random numbers between 1381 and 1453 and multiply them together, reporting the result. \"},\n]\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\n# outputs = pipeline(\n#     messages,\n#     max_new_tokens=256,\n#     eos_token_id=terminators,\n#     do_sample=True,\n#     temperature=0.6,\n#     top_p=0.9,\n# )\n# print(outputs[0][\"generated_text\"][-1][\"content\"])\n# print(outputs[1][\"generated_text\"][-1][\"content\"])\n\noutputs = []\n\n# Loop over each message to query the model separately\nfor message in messages:\n    result = pipeline(\n        [message],  # Process each message individually\n        max_new_tokens=256,\n        eos_token_id=terminators,\n        do_sample=True,\n        temperature=0.6,\n        top_p=0.9,\n    )\n    # Assuming the model returns a list of dictionaries, append the response to outputs\n    outputs.append(result[0][\"generated_text\"])\n\n# Print the responses for each prompt\nfor i, output in enumerate(outputs):\n    print(f\"Response to prompt {i+1}:\")\n    print(output)\n    print()\n\n# for i, output in enumerate(outputs):\n#     print(f\"Response to prompt {i+1}:\")\n#     print(output[0][\"generated_text\"][-1][\"content\"])\n#     print()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VgjfzXHohLTF","outputId":"2852f799-cc4e-4cb2-fdd7-90975d39b049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"},{"output_type":"stream","name":"stdout","text":"Response to prompt 1:\n\n[{'role': 'user', 'content': 'which is bigger, 9.11 and 9.9'}, {'role': 'assistant', 'content': '9.11 is bigger than 9.9.'}]\n\n\n\nResponse to prompt 2:\n\n[{'role': 'user', 'content': 'Select two random numbers between 1381 and 1453 and multiply them together, reporting the result. '}, {'role': 'assistant', 'content': \"I've randomly selected two numbers between 1381 and 1453:\\n\\nNumber 1: 1412\\nNumber 2: 1419\\n\\nMultiplying them together gives:\\n\\n1412 × 1419 = 2,011,408\"}]\n\n\n"}]},{"cell_type":"code","source":"print(outputs[0][\"generated_text\"])\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ietEvDDDYuOb","outputId":"9c7282a5-e7ee-4f82-ff30-66df77988dba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"[{'role': 'user', 'content': 'which is bigger, 9.11 and 9.9'}, {'role': 'user', 'content': 'Select two random numbers between 1381 and 1453 and multiply them together, reporting the result. '}, {'role': 'assistant', 'content': \"I've generated two random numbers between 1381 and 1453:\\n\\n* Number 1: 1402\\n* Number 2: 1411\\n\\nMultiplying them together gives:\\n\\n1402 × 1411 = 1,980,422\"}]\n"}]},{"cell_type":"code","source":"# Usage\nimport torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\ntokenizer = LlamaTokenizer.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base')\nmodel = LlamaForCausalLM.from_pretrained('sarvamai/OpenHathi-7B-Hi-v0.1-Base', torch_dtype=torch.bfloat16)\n\nprompt = \"मैं एक अच्छा हाथी हूँ\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Generate\ngenerate_ids = model.generate(inputs.input_ids, max_length=30)\ntokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159,"referenced_widgets":["4e16efbd44744c4fbfdc047af35f6f89","5f901426e7d444bfb3a9f88dc3c2b0f0","cd73176cf66b4f7593f22e65cbb54849","1e284d16da18460b998d8b315b5df088","d208d9405a5d46c186e362c58816df45","a570885d9b7e4d77ba1e3d93e435d51d","398878ca09d0416685b1a53df3aecdba","b7ded2597e974f3b884f1110506d0358","6b2ff7ebada74163a0b0477fc047954a","b6209f9d51454514bce02663c9df4561","fd5838fbd81149c1a82a95b5c3a850b2"]},"id":"A9V_OJ4YOSlZ","outputId":"f747e244-80bf-4a95-b254-eaa4f63f4405"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n\nThe secret `HF_TOKEN` does not exist in your Colab secrets.\n\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n\nYou will be able to reuse this secret in all of your notebooks.\n\nPlease note that authentication is recommended but still optional to access public models or datasets.\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e16efbd44744c4fbfdc047af35f6f89"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"32mtCA9FTQ_7","outputId":"bca4df0d-7465-4bb9-a1f4-23b53c52c7c0"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pipeline' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2eae459dff4c>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Extract embeddings for prompts from the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mclassification_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classification_prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextract_final_token_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassification_prompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Print the embeddings (example for the first 5 prompts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-2eae459dff4c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Extract embeddings for prompts from the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mclassification_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classification_prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextract_final_token_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassification_prompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Print the embeddings (example for the first 5 prompts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"]}]}]}