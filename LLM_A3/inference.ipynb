{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-04T09:50:40.610397Z","iopub.status.busy":"2024-11-04T09:50:40.609405Z","iopub.status.idle":"2024-11-04T09:52:00.435891Z","shell.execute_reply":"2024-11-04T09:52:00.434991Z","shell.execute_reply.started":"2024-11-04T09:50:40.610354Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n","Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3798647b723c4cf1b140c07f14591f46","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of PhiForSequenceClassification were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["!pip install -q bitsandbytes datasets accelerate loralib\n","!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n","!pip install evaluate scikit-learn\n","import torch\n","from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from datasets import load_dataset, Dataset, DatasetDict\n","from peft import PeftModel, PeftConfig\n","from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=False,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model_name='microsoft/phi-2'\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map={\"\":0},\n","    trust_remote_code=True,\n","    num_labels=3,\n","    low_cpu_mem_usage=True\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","ft_model = PeftModel.from_pretrained(model, \"/kaggle/input/phi2-inference/kaggle/working/peft-snli/final-checkpoint/checkpoint-315\",torch_dtype=torch.float16,is_trainable=False)\n","ft_model.config.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:52:00.438696Z","iopub.status.busy":"2024-11-04T09:52:00.438114Z","iopub.status.idle":"2024-11-04T09:52:03.790561Z","shell.execute_reply":"2024-11-04T09:52:03.789714Z","shell.execute_reply.started":"2024-11-04T09:52:00.438644Z"},"trusted":true},"outputs":[],"source":["data = load_dataset(\"stanfordnlp/snli\")\n","train_samples = Dataset.from_dict(data['train'].select(range(0, 550152, 550))[:1000])\n","test_samples = Dataset.from_dict(data['test'].select(range(0, 10000, 100))[:100])\n","validation_samples = Dataset.from_dict(data['validation'].select(range(0, 10000, 100))[:100] )"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:52:03.792044Z","iopub.status.busy":"2024-11-04T09:52:03.791690Z","iopub.status.idle":"2024-11-04T09:52:16.762453Z","shell.execute_reply":"2024-11-04T09:52:16.761491Z","shell.execute_reply.started":"2024-11-04T09:52:03.792002Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!pip install -q evaluate\n","import evaluate\n","import pandas as pd\n","from tqdm import tqdm\n","metric = evaluate.load(\"accuracy\")\n","def infer_tuned(sample):\n","\tpremise, hypothesis, label = sample['premise'], sample['hypothesis'], sample['label']\n","\tinputs = tokenizer(premise, hypothesis, return_tensors='pt', padding='max_length', truncation=True, max_length=128).to(model.device)\n","\twith torch.no_grad():\n","\t\toutputs = ft_model(**inputs)\n","\t\tlogits = outputs.logits\n","\t\tpredictions = torch.argmax(logits, dim=1).item()\n","                \n","\tlabel_dict = {0: 'entailment', 1: 'neutral', 2: 'contradiction', -1: 'unknown'}\n","\treturn predictions, label\n","\n","def compute_accuracy_tuned(dataset):\n","    predictions = []\n","    labels, results = [], []\n","    for sample in tqdm(dataset):\n","        prediction, label = infer_tuned(sample)\n","        predictions.append(prediction)\n","        labels.append(label)\n","        results.append({\"sample\": sample, \"prediction\": prediction, \"label\": label})\n","    df = pd.DataFrame(results)\n","    df.to_csv(\"finetuned_predictions.csv\", index=False)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:52:16.765073Z","iopub.status.busy":"2024-11-04T09:52:16.764018Z","iopub.status.idle":"2024-11-04T09:52:20.512669Z","shell.execute_reply":"2024-11-04T09:52:20.511629Z","shell.execute_reply.started":"2024-11-04T09:52:16.765013Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea01ba6108a44bd0b8579185dc18a953","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of PhiForSequenceClassification were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["pretrained_model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map={\"\":0},\n","    trust_remote_code=True,\n","    num_labels=3,\n","    low_cpu_mem_usage=True\n",")\n","tokenizer_pre = AutoTokenizer.from_pretrained(model_name)\n","tokenizer_pre.pad_token = tokenizer_pre.eos_token"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:52:20.516932Z","iopub.status.busy":"2024-11-04T09:52:20.516141Z","iopub.status.idle":"2024-11-04T09:52:20.526704Z","shell.execute_reply":"2024-11-04T09:52:20.525740Z","shell.execute_reply.started":"2024-11-04T09:52:20.516895Z"},"trusted":true},"outputs":[],"source":["def infer_pretrained(sample):\n","\tpremise, hypothesis, label = sample['premise'], sample['hypothesis'], sample['label']\n","\tinputs = tokenizer_pre(premise, hypothesis, return_tensors='pt', padding='max_length', truncation=True, max_length=128).to(model.device)\n","\twith torch.no_grad():\n","\t\toutputs = pretrained_model(**inputs)\n","\t\tlogits = outputs.logits\n","\t\tpredictions = torch.argmax(logits, dim=1).item()\n","                \n","\tlabel_dict = {0: 'entailment', 1: 'neutral', 2: 'contradiction', -1: 'unknown'}\n","\treturn predictions, label\n","\n","def compute_accuracy_pretrained(dataset):\n","    predictions = []\n","    labels = []\n","    results = []\n","    for sample in tqdm(dataset):\n","        prediction, label = infer_pretrained(sample)\n","        predictions.append(prediction)\n","        labels.append(label)\n","        results.append({\"sample\": sample, \"prediction\": prediction, \"label\": label})\n","    df = pd.DataFrame(results)\n","    df.to_csv(\"pretrained_predictions.csv\", index=False)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:52:20.528183Z","iopub.status.busy":"2024-11-04T09:52:20.527875Z","iopub.status.idle":"2024-11-04T09:52:49.035817Z","shell.execute_reply":"2024-11-04T09:52:49.034916Z","shell.execute_reply.started":"2024-11-04T09:52:20.528151Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:28<00:00,  3.51it/s]\n"]},{"data":{"text/plain":["{'accuracy': 0.36}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["compute_accuracy_pretrained(test_samples)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:52:49.037265Z","iopub.status.busy":"2024-11-04T09:52:49.036967Z","iopub.status.idle":"2024-11-04T09:53:19.603615Z","shell.execute_reply":"2024-11-04T09:53:19.602664Z","shell.execute_reply.started":"2024-11-04T09:52:49.037232Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n"]},{"data":{"text/plain":["{'accuracy': 0.85}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["compute_accuracy_tuned(test_samples)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:53:19.605721Z","iopub.status.busy":"2024-11-04T09:53:19.605048Z","iopub.status.idle":"2024-11-04T09:53:48.073607Z","shell.execute_reply":"2024-11-04T09:53:48.072718Z","shell.execute_reply.started":"2024-11-04T09:53:19.605673Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:28<00:00,  3.52it/s]\n"]},{"data":{"text/plain":["{'accuracy': 0.33}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["compute_accuracy_pretrained(validation_samples)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:53:48.075020Z","iopub.status.busy":"2024-11-04T09:53:48.074680Z","iopub.status.idle":"2024-11-04T09:54:18.639412Z","shell.execute_reply":"2024-11-04T09:54:18.638388Z","shell.execute_reply.started":"2024-11-04T09:53:48.074987Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:30<00:00,  3.27it/s]\n"]},{"data":{"text/plain":["{'accuracy': 0.82}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["compute_accuracy_tuned(validation_samples)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:54:18.640857Z","iopub.status.busy":"2024-11-04T09:54:18.640527Z","iopub.status.idle":"2024-11-04T09:59:03.285330Z","shell.execute_reply":"2024-11-04T09:59:03.284379Z","shell.execute_reply.started":"2024-11-04T09:54:18.640821Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000/1000 [04:44<00:00,  3.51it/s]\n"]},{"data":{"text/plain":["{'accuracy': 0.32}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["compute_accuracy_pretrained(train_samples)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T09:59:03.286890Z","iopub.status.busy":"2024-11-04T09:59:03.286573Z","iopub.status.idle":"2024-11-04T10:04:08.755366Z","shell.execute_reply":"2024-11-04T10:04:08.754418Z","shell.execute_reply.started":"2024-11-04T09:59:03.286856Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000/1000 [05:05<00:00,  3.27it/s]\n"]},{"data":{"text/plain":["{'accuracy': 0.943}"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["compute_accuracy_tuned(train_samples)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-11-04T10:08:41.092393Z","iopub.status.busy":"2024-11-04T10:08:41.091971Z","iopub.status.idle":"2024-11-04T10:08:41.465098Z","shell.execute_reply":"2024-11-04T10:08:41.464261Z","shell.execute_reply.started":"2024-11-04T10:08:41.092354Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 18350080 || all params: 1408634880 || trainable%: 1.3026853346127565\n"]}],"source":["from peft import prepare_model_for_kbit_training\n","\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)\n","def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","from peft import LoraConfig, get_peft_model\n","\n","config = LoraConfig(\n","    r=16,\n","    lora_alpha=64,\n","    lora_dropout=0.05,\n","    bias=\"none\"\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6004487,"sourceId":9797702,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
